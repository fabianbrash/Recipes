
######My kubernetes journey#################################
##REF:https://kubernetes.io/docs/tasks/
##REF:https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#-strong-api-overview-strong-
##REF:https://kubernetes.io/blog/2019/07/18/api-deprecations-in-1-16/
##REF:https://kubernetes.io/docs/reference/


kubectl version
kubectl version --short
kubectl api-versions

kubectl get apiservices

kubectl get apiservice| grep False

kubectl version --client --short

kubectl get namespaces
kubectl get namespaces -A ##get all namespaces even system namespaces by default all your pods are created in the default namespace
kubectl get pods
kubectl get deployments
kubectl get services or kubectl get svc

kubectl get all -o wide

watch kubectl get all -o wide

kubectl get events # get all events


## Label a node###
kubectl label node mymode1 ssdpresent=true

##find nodes with a label 
kubectl get nodes -l ssdpresent=true 

kubectl get node node1 --show-labels

kubectl get ingress
#or 
kubectl get ing 
kubectl describe ing 



##Get current context
kubectl config view
kubectl config current-context

kubectl config get-contexts

#kubectl get - list resources
#kubectl describe - show detailed information about a resource
#kubectl logs - print the logs from a container in a pod
#kubectl exec - execute a command on a container in a pod

####After you create a deployment you can create a service so external users can access it
##But first after we deploy our deployment lets get the IP of the pods
kubectl get pods -o wide

###or to specify a label from our deployment
kubectl get pods -l app=nginx -o wide

##you can expose the service with
kubectl expose deployment/nginx-deployment

##Or you can do 
kubectl expose deployment mydeployment --type=NodePort

##or with a yaml file check my yaml repo

####Run a simple deployment without a yaml file
kubectl run kubernetes-bootcamp --image=gcr.io/google-samples/kubernetes-bootcamp:v1 --port=8080

kubectl run myshell -it --image busybox -- sh 

kubectl run myshell -it --rm --image busybox -- sh 

#Also
kubectl create deployment ng1 --image=nginx:stable-alpine

kubectl scale deployment ng1 --replicas 2

kubectl run ng1 --image=nginx --replicas 2

##OR the new way

kubectl run --generator=run-pod/v1 busybox --image=busybox:1.28

#then we can do this
kubectl expose deploy ng1 --port 80 --type= NodePort


##delete the deployment 
kubectl get deployments
kubectl delete deployement 'our deployment name'

##Also let's delete deployment and service for ng1 and ng2##
kubectl delete deployment,service ng1 ng2

kubectl delete pod mypod 
#note the above only works if you created the pod with
#a yaml file and the type was pod, if you use
#kubectl run ... this actually creates a deployment
#on the fly so you will have to use kubectl delete deployment 


###you can run the kubectl proxy command but it's best to look at services
kubectl proxy ##run in a different terminal
curl http://localhost:8001/version

kubectl port-forward podname 8080:80


###delete deployment
kubectl delete deployment my-deployment

###Create a custom namespace#############
#REF:https://www.assistanz.com/steps-to-create-custom-namespace-in-the-kubernetes/
kubectl create namespace mynamespace

##services##
#we can edit a service with
kubectl edit svc ng1
#also if we are editing a service in a different namespace 
kubectl -n mynamespace edit svc ng1 

kubectl -n kube-system get pod coredns-345ab -o yaml 

kubectl edit namespace dev

kubectl get cs ##component status

kubectl get pv # get persistence volume

kubectl exec pod/container name ls 


###YAML###########
apiVersion: v1
kind: Namespace
metadata:
   name: mynamespace
   
##END YAML

###Deploy a pod into our namespace
kubectl run ns-pod --image=nginx --port=80 --generator=run-pod/v1 -n mynamespace

kubectl get pods --namespace mynamespace

##Delete the pods in the namespace
kubectl delete pods --all --namespace mynamespace

##Delete namespace
kubectl delete namespace mynamespace

kubectl describe podname 

#get all pods with the label run=ng1
kubectl get pods -l run=ng1 

kubectl scale deployment ng1 --replicas=3

##To get kubectl up and running you need to copy the admin.conf file from
##/etc/kubernetes on one of the master nodes into a directory ~/.kube
##in your home directory and then you need to rename admin.conf to config

##the above will scale our deployment but u should update your YAML file


###Add to our .kube/config so we can switch namespaces
#REF:https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/

##Get current context
kubectl config view
kubectl config current-context

##You populate the below from the results of the 2 commands above

kubectl config set-context dev --namespace=development \
  --cluster=lithe-cocoa-92103_kubernetes \
  --user=lithe-cocoa-92103_kubernetes

kubectl config set-context prod --namespace=production \
  --cluster=lithe-cocoa-92103_kubernetes \
  --user=lithe-cocoa-92103_kubernetes


kubectl config view

kubectl config use-context dev

kubectl config current-context

####Now switch to prod
kubectl config use-context prod

###Rolling upgrades#####

kubectl rollout status deployment my-deploy

kubectl rollout history deploy my-deploy

kubectl set image deploy my-deploy nginx=1.15  ##upgrade the version of nginx in our deployment

kubectl annotate deploy my-deploy kubernetes.io/change-cause="Updated to version 1.15" ## annotate the change

##Or###
kubectl set image deploy my-deploy nginx=1.15 --record

kubectl rollout history deploy my-deploy --revision 1  ##lookup revision 1

##Undo a rolling upgrade and specify to what revision

kubectl rollout undo deploy my-deploy --to-revision=2 

###Pause rollout

kubectl rollout pause deploy my-deploy
kubectl rollout resume deploy my-deploy



#########################KUBERNETES INSTALL NOTES####################################################
###I will turn this into a bash script soon and then hopefully an ansible playbook####
##Thanks to REF:https://github.com/justmeandopensource/kubernetes/blob/master/docs/install-cluster.md

su -

##edit hosts file
cat >>/etc/hosts<<EOF
192.168.50.104 k8master.itbu.ad.enterprise.com k8master
192.168.50.105 k8worker01.itbu.ad.enterprise.com k8worker01
EOF

##Install docker
yum install -y yum-utils device-mapper-persistent-data lvm2

yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo

yum install -y docker-ce

systemctl enable docker
systemctl start docker

##disable selinux
setenforce 0
sed -i --follow-symlinks 's/^SELINUX=enforcing/SELINUX=disabled/' /etc/sysconfig/selinux

##Ubuntu 18.04
setenforce 0
sed -i --follow-symlinks 's/^SELINUX=enforcing/SELINUX=disabled/' /etc/selinux/config

##By defualt it seems 18.04 is set to permissive which should work for us

##Disable firewalld
systemctl disable firewalld
systemctl stop firewalld

##Disable swap
sed -i '/swap/d' /etc/fstab
swapoff -a

##Update sysctl settings for Kubernetes networking

cat >>/etc/sysctl.d/kubernetes.conf<<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system


###Note the above is not required on Debian based systems i.e. Ubuntu, to make sure 
sysctl net.bridge.bridge-nf-call-iptables
sysctl net.bridge.bridge-nf-call-ip6tables


##Kubernetes Setup

##Add yum repository
##Note the indentation for gpgkey is vital or this will fail installing kubelet

cat >>/etc/yum.repos.d/kubernetes.repo<<EOF
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
        https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

##Install kubernetes
yum install -y kubeadm kubelet kubectl

## Enable and Start kubelet service
systemctl enable kubelet
systemctl start kubelet


##Initialize Kubernetes Cluster

kubeadm init --apiserver-advertise-address=192.168.50.104 --pod-network-cidr=10.244.0.0/16


##Run the command as root our using sudo

kubeadm join 192.168.50.104:6443 --token jpffga.v6rwru1tv5euqj2n \
    --discovery-token-ca-cert-hash sha256:df00931ffc172d6206eeccf9edf40ca632e7ce6ebeed2eab4e5b2989b4481976



###Copy kube config
##To be able to use kubectl command to connect and interact with the cluster, the user needs kube config file.

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

##Deploy flannel network
##this is done with the above user
##Note if you have more than 1 interfaces then you will have to first download the yml file and make an edit to it
## find --iface and set to eth1(or whatever)
##modified yml file @ 


####Look for
containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.10.0-amd64
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        - --iface=eth1
###################################################################



https://raw.githubusercontent.com/justmeandopensource/kubernetes/master/vagrant-provisioning/kube-flannel.yml

kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml



###Cluster join command
##If you didn't save the output from the cluster creation
kubeadm token create --print-join-command


##ALL done

kubectl get pods
kubectl version --short
kubectl get componentstatus or kubectl get cs


###kubectl run interactive####
kubectl run -i --tty busybox --image=busybox --restart=Never -- sh

##The above is equivalent to docker run -it busybox sh

## this also works

kubectl exec -it nfs-nginx-766d4bf45f-jn4b6 bash

## also 

kubectl run myshell -it --rm --image=busybox -- sh # this gave me issues but the above worked

##Be careful you don't create a deployment instead of a pod note if you create a deployment
## then the pod won't be called myshell but instead myshell-xyz so you can't shell into myshell
## because it doesn't exist


##Drain a node######
kubectl drain worker-0


###The difference between port and target port
##REF:https://matthewpalmer.net/kubernetes-app-developer/articles/kubernetes-ports-targetport-nodeport-service.html

## obviously NodePort: xxx is the port we use to access our service from outside of our k8s cluster
## "port" is the port that services inside the cluster can communicate on to your svc
## and "targetPort" is the port that the container is actually listening on
## Example

ports:
  - nodePort: 32321
    port: 443
    targetPort: 8443


##The above sample says we can access the svc externally on 32321 i.e. http://worker-0:32321
## the "port" says within the k8s cluster other pods/deployments/anything can access on port 443
## and "targetPort" is the actaully exposed port in the container 8443 so if another pod 
## within the cluster accesses the deployment/pod on 443 it should roll to 8443 on the target pod(s)

####Create a config file for a user

kubectl --kubeconfig john.kubeconfig config set-cluster mycluster --server https://IP_OF_API_SERVER:6443 --certificate-authority=ca.crt
kubectl --kubeconfig john.kubeconfig config set-credentials john --client-certificate /home/john/john.crt -client-key /home/john/john.key
kubectl --kubeconfig john.kubeconfig config set-context john-kubernetes --cluster mycluster --namespace finance --user john

##Note if you look at john.kubeconfig and the current-context is not set you need to set it
current-context: john-kubernetes
##and then save it and give it to john

##Create a role

kubectl create role john-finance --verb=get --verb=list --verb=watch --resource=pods --namespace=finance

##Or

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: finance
  name: john-finance
rules:
- apiGroups: [""] # "" indicates the core API group
  resources: ["pods"]
  verbs: ["get", "watch", "list"]



kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  namespace: finance
  name: john-finance
rules:
- apiGroups: ["", "extensions", "apps"]
  resources: ["deployments", "replicasets", "pods"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"] # You can also use ["*"]
  
####Create a role-binding

kubectl create rolebinding john-finance-rolebinding --role=john-finance --user=john --namespace=finance

##Or

kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: john-finance-rolebinding
  namespace: finance
subjects:
- kind: User
  name: john
  apiGroup: ""
roleRef:
  kind: Role
  name: john-finance
  apiGroup: ""

##Note a role called john-finance has to exist to do this


######So I had an interesting issue when deploying golang and alpine in my k82 cluster
###the image would get pulled and then it would go into a crash loop, but if I ran
## and nginx or a mariadb pod, everything was fine, then it hit me thise images are minimal
##the do nothing unlike docker where you would see 'exited' k8s just goes in a loop trying to
##restart the pod(odd since this was a pod spec why didn't it just start and then terminate the pod, maybe
##the default restart policy is in play here) anyways the fix was to run the pod with a sleep of 10000 and 
#viola it worked.


###K8s RBAC

kubectl auth can-i create pods
kubectl auth can-i create services
kubectl auth can-i create deployments

kubectl auth can-i create pods --as=john
kubectl auth can-i list pods --as=john

##You get the idea



####Kubernetes secrets###
##REF:https://kubernetes.io/docs/concepts/configuration/secret/#creating-a-secret-manually

###PAY VERY CLOSE ATTENTION TO HOW THE PASSWORD IS ENCODED FROM THE ABOVE REF 

echo -n 'admin' | base64

echo -n '1f2d1e2e67df' | base64


####Kubernetes has the ability to sign certificates
##ReF:https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/

kubectl get csr --all-namespaces

####NOTE THE -n WITHOUT WHICH STANDS FOR 'do not output trailing new line'
### WITHOUT THAT YOU'RE PASSWORDS WILL BE WRONG I USED THIS FOR A MARIADB INSTANCE
## AND I MADE THE MISTAKE OF NOT USING -n AND EVERY TIME I ATTEMPTED TO LOGIN IN A GOT
##INCORRECT PASSWORD


####TLS Secrets####

kubectl create secret tls ${CERT_NAME} --key ${KEY_FILE} --cert ${CERT_FILE}

##example
kubectl create secret tls my_cert --key cert-key.pem --cert cert.pem




##Contexts#################
##REF:https://kubernetes.io/docs/reference/kubectl/cheatsheet/#kubectl-context-and-configuration
##REF:https://ahmet.im/blog/mastering-kubeconfig/
##REF:https://www.jacobtomlinson.co.uk/posts/2019/how-to-merge-kubernetes-kubectl-config-files/

kubectl config view # Show Merged kubeconfig settings.

# use multiple kubeconfig files at the same time and view merged config
KUBECONFIG=~/.kube/config:~/.kube/kubconfig2 

kubectl config view

# get the password for the e2e user
kubectl config view -o jsonpath='{.users[?(@.name == "e2e")].user.password}'

kubectl config view -o jsonpath='{.users[].name}'    # display the first user
kubectl config view -o jsonpath='{.users[*].name}'   # get a list of users
kubectl config get-contexts                          # display list of contexts 
kubectl config current-context                       # display the current-context
kubectl config use-context my-cluster-name           # set the default context to my-cluster-name

# add a new user to your kubeconf that supports basic auth
kubectl config set-credentials kubeuser/foo.kubernetes.com --username=kubeuser --password=kubepassword

# permanently save the namespace for all subsequent kubectl commands in that context.
kubectl config set-context --current --namespace=ggckad-s2

# set a context utilizing a specific username and namespace.
kubectl config set-context gce --user=cluster-admin --namespace=foo \
  && kubectl config use-context gce
 
kubectl config unset users.foo # delete user foo



##Let's merge all of our config files to 1 master 1, make sure to backup your config file first

KUBECONFIG=~/.kube/config:/path/to/new/config kubectl config view --flatten > ~/.kube/config


##Duplicate users

## You will also probably also run into clusters with the same name and if you attempt to switch
##everything will looke fine until you run the kubectl command and you get unauthorized error
##it depends on the order you combined your file, the first 1 will win so you will be able to connect to
##the first cluster but get an error for the second, to fix just do the below
##REF:https://imti.co/kubectl-remote-context/

contexts:
- context:
    cluster: leave as is
    user: change to something else(I use the cluster name)
  name: user@leave original
  ....
  
  users:
  - name: change to the same as user
  ...
  
  
  ##Example
  
  contexts:
- context:
    cluster: leave as is
    user: gke-0
  name: gke-0@leave original
  ....
  
  users:
  - name: gke-0
  ...
  



####access api via kubectl

kubectl get --raw "/apis/metrics.k8s.io/v1beta1/nodes"


##show labels

kubectl get ns default --show-labels


####rollout & restart

kubectl rollout restart deploy my-deploy

kubectl rollout restart -n mynamespace

##Annotate a ns

kubectl annotate -n default release=dev


###Autoscaling
#REF:https://cloud.google.com/kubernetes-engine/docs/how-to/horizontal-pod-autoscaling#api-versions
#REF:https://cloud.google.com/kubernetes-engine/docs/how-to/vertical-pod-autoscaling

##v2beta2

kubectl edit hpa.v2beta2.autoscaling myhpa

kubectl get hpa.v2beta2.autoscaling myhpa


##bash completion
##REF:https://kubernetes.io/docs/reference/kubectl/cheatsheet/#kubectl-autocomplete


source <(kubectl completion bash) # setup autocomplete in bash into the current shell, bash-completion package should be installed first.
echo "source <(kubectl completion bash)" >> ~/.bashrc # add autocomplete permanently to your bash shell.



####So I had an issue where I was deleted some namespaces but it just stayed in a terminationg state
##I left over night and still they were stuck,so after a little Googling I found the below
##REF:https://medium.com/@craignewtondev/how-to-fix-kubernetes-namespace-deleting-stuck-in-terminating-state-5ed75792647e

kubectl get namespace fixme -o json > fixme.json

##Then find "kubernetes" under "finalizers" and delete it("kubernetes") leave finalizers array empty

##Then

kubectl replace --raw "/api/v1/namespaces/fixme/finalize" -f fixme.json

###Note according to the article this should work on other resources including pods,deployments,services,etc.



###Remove linkerd from deployments
#REF: https://linkerd.io/2/reference/cli/uninject/

# Uninject all the deployments in the default namespace.
kubectl get deploy -o yaml | linkerd uninject - | kubectl apply -f -

# Download a resource and uninject it through stdin.
curl http://url.to/yml | linkerd uninject - | kubectl apply -f -

# Uninject all the resources inside a folder and its sub-folders.
linkerd uninject <folder> | kubectl apply -f -

###Troubleshoting####

kubectl api-resources --verbs=list --namespaced -o name | xargs -n 1 kubectl get --show-kind --show-all --ignore-not-found -n test-ns

## The above might also generate a unknown flag: --show-all


##But we are looking for below, which looks like linkerd is causing our issue

#error: unable to retrieve the complete list of server APIs: packages.operators.coreos.com/v1: 
#the server is currently unable to handle the request, tap.linkerd.io/v1alpha1: the server is currently unable to handle the request


kubectl api-resources --verbs=list --namespaced -o name | xargs -n 1 kubectl get -n test-ns

kubectl api-resources --verbs=list --namespaced -o name | xargs -n 1 kubectl get --show-kind --show-all --ignore-not-found -n test-ns

#Then from the out put above

kubectl get APIService v1.packages.operators.coreos.com

##This was causing my cluster to not delete namespaces
kubectl delete APIService v1alpha1.tap.linkerd.io

