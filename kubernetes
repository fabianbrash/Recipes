
######My kubernetes journey#################################
##REF:https://kubernetes.io/docs/tasks/
##REF:https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#-strong-api-overview-strong-
##REF:https://kubernetes.io/blog/2019/07/18/api-deprecations-in-1-16/


kubectl version
kubectl version --short
kubectl api-versions

kubectl version --client --short

kubectl get namespaces
kubectl get namespaces -A ##get all namespaces even system namespaces by default all your pods are created in the default namespace
kubectl get pods
kubectl get deployments
kubectl get services or kubectl get svc

kubectl get all -o wide

watch kubectl get all -o wide

kubectl get events # get all events


## Label a node###
kubectl label node mymode1 ssdpresent=true

##find nodes with a label 
kubectl get nodes -l ssdpresent=true 

kubectl get node node1 --show-labels

kubectl get ingress
#or 
kubectl get ing 
kubectl describe ing 



##Get current context
kubectl config view
kubectl config current-context

kubectl config get-contexts

#kubectl get - list resources
#kubectl describe - show detailed information about a resource
#kubectl logs - print the logs from a container in a pod
#kubectl exec - execute a command on a container in a pod

####After you create a deployment you can create a service so external users can access it
##But first after we deploy our deployment lets get the IP of the pods
kubectl get pods -o wide

###or to specify a label from our deployment
kubectl get pods -l app=nginx -o wide

##you can expose the service with
kubectl expose deployment/nginx-deployment

##Or you can do 
kubectl expose deployment mydeployment --type=NodePort

##or with a yaml file check my yaml repo

####Run a simple deployment without a yaml file
kubectl run kubernetes-bootcamp --image=gcr.io/google-samples/kubernetes-bootcamp:v1 --port=8080

kubectl run myshell -it --image busybox -- sh 

kubectl run myshell -it --rm --image busybox -- sh 

#Also
kubectl create deployment ng1 --image=nginx:stable-alpine

kubectl scale deployment ng1 --replicas 2

kubectl run ng1 --image=nginx --replicas 2

##OR the new way

kubectl run --generator=run-pod/v1 busybox --image=busybox:1.28

#then we can do this
kubectl expose deploy ng1 --port 80 --type= NodePort


##delete the deployment 
kubectl get deployments
kubectl delete deployement 'our deployment name'

##Also let's delete deployment and service for ng1 and ng2##
kubectl delete deployment,service ng1 ng2

kubectl delete pod mypod 
#note the above only works if you created the pod with
#a yaml file and the type was pod, if you use
#kubectl run ... this actually creates a deployment
#on the fly so you will have to use kubectl delete deployment 


###you can run the kubectl proxy command but it's best to look at services
kubectl proxy ##run in a different terminal
curl http://localhost:8001/version

kubectl port-forward podname 8080:80


###delete deployment
kubectl delete deployment my-deployment

###Create a custom namespace#############
#REF:https://www.assistanz.com/steps-to-create-custom-namespace-in-the-kubernetes/
kubectl create namespace mynamespace

##services##
#we can edit a service with
kubectl edit svc ng1
#also if we are editing a service in a different namespace 
kubectl -n mynamespace edit svc ng1 

kubectl -n kube-system get pod coredns-345ab -o yaml 

kubectl edit namespace dev

kubectl get cs ##component status

kubectl get pv # get persistence volume

kubectl exec pod/container name ls 


###YAML###########
apiVersion: v1
kind: Namespace
metadata:
   name: mynamespace
   
##END YAML

###Deploy a pod into our namespace
kubectl run ns-pod --image=nginx --port=80 --generator=run-pod/v1 -n mynamespace

kubectl get pods --namespace mynamespace

##Delete the pods in the namespace
kubectl delete pods --all --namespace mynamespace

##Delete namespace
kubectl delete namespace mynamespace

kubectl describe podname 

#get all pods with the label run=ng1
kubectl get pods -l run=ng1 

kubectl scale deployment ng1 --replicas=3

##To get kubectl up and running you need to copy the admin.conf file from
##/etc/kubernetes on one of the master nodes into a directory ~/.kube
##in your home directory and then you need to rename admin.conf to config

##the above will scale our deployment but u should update your YAML file


###Add to our .kube/config so we can switch namespaces
#REF:https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/

##Get current context
kubectl config view
kubectl config current-context

##You populate the below from the results of the 2 commands above

kubectl config set-context dev --namespace=development \
  --cluster=lithe-cocoa-92103_kubernetes \
  --user=lithe-cocoa-92103_kubernetes

kubectl config set-context prod --namespace=production \
  --cluster=lithe-cocoa-92103_kubernetes \
  --user=lithe-cocoa-92103_kubernetes


kubectl config view

kubectl config use-context dev

kubectl config current-context

####Now switch to prod
kubectl config use-context prod

###Rolling upgrades#####

kubectl rollout status deployment my-deploy

kubectl rollout history deploy my-deploy

kubectl set image deploy my-deploy nginx=1.15  ##upgrade the version of nginx in our deployment

kubectl annotate deploy my-deploy kubernetes.io/change-cause="Updated to version 1.15" ## annotate the change

##Or###
kubectl set image deploy my-deploy nginx=1.15 --record

kubectl rollout history deploy my-deploy --revision 1  ##lookup revision 1

##Undo a rolling upgrade and specify to what revision

kubectl rollout undo deploy my-deploy --to-revision=2 

###Pause rollout

kubectl rollout pause deploy my-deploy
kubectl rollout resume deploy my-deploy



#########################KUBERNETES INSTALL NOTES####################################################
###I will turn this into a bash script soon and then hopefully an ansible playbook####
##Thanks to REF:https://github.com/justmeandopensource/kubernetes/blob/master/docs/install-cluster.md

su -

##edit hosts file
cat >>/etc/hosts<<EOF
192.168.50.104 k8master.itbu.ad.enterprise.com k8master
192.168.50.105 k8worker01.itbu.ad.enterprise.com k8worker01
EOF

##Install docker
yum install -y yum-utils device-mapper-persistent-data lvm2

yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo

yum install -y docker-ce

systemctl enable docker
systemctl start docker

##disable selinux
setenforce 0
sed -i --follow-symlinks 's/^SELINUX=enforcing/SELINUX=disabled/' /etc/sysconfig/selinux

##Disable firewalld
systemctl disable firewalld
systemctl stop firewalld

##Disable swap
sed -i '/swap/d' /etc/fstab
swapoff -a

##Update sysctl settings for Kubernetes networking

cat >>/etc/sysctl.d/kubernetes.conf<<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system


##Kubernetes Setup

##Add yum repository
##Note the indentation for gpgkey is vital or this will fail installing kubelet

cat >>/etc/yum.repos.d/kubernetes.repo<<EOF
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
        https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF

##Install kubernetes
yum install -y kubeadm kubelet kubectl

## Enable and Start kubelet service
systemctl enable kubelet
systemctl start kubelet


##Initialize Kubernetes Cluster

kubeadm init --apiserver-advertise-address=192.168.50.104 --pod-network-cidr=10.244.0.0/16


##Run the command as root our using sudo

kubeadm join 192.168.50.104:6443 --token jpffga.v6rwru1tv5euqj2n \
    --discovery-token-ca-cert-hash sha256:df00931ffc172d6206eeccf9edf40ca632e7ce6ebeed2eab4e5b2989b4481976



###Copy kube config
##To be able to use kubectl command to connect and interact with the cluster, the user needs kube config file.

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

##Deploy flannel network
##this is done with the above user
##Note if you have more than 1 interfaces then you will have to first download the yml file and make an edit to it
## find --iface and set to eth1(or whatever)
##modified yml file @ 


####Look for
containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.10.0-amd64
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        - --iface=eth1
###################################################################



https://raw.githubusercontent.com/justmeandopensource/kubernetes/master/vagrant-provisioning/kube-flannel.yml

kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml



###Cluster join command
##If you didn't save the output from the cluster creation
kubeadm token create --print-join-command


##ALL done

kubectl get pods
kubectl version --short
kubectl get componentstatus or kubectl get cs


###kubectl run interactive####
kubectl run -i --tty busybox --image=busybox --restart=Never -- sh

##The above is equivalent to docker run -it busybox sh

## this also works

kubectl exec -it nfs-nginx-766d4bf45f-jn4b6 bash

## also 

kubectl run myshell -it --rm --image=busybox -- sh # this gave me issues but the above worked

##Be careful you don't create a deployment instead of a pod note if you create a deployment
## then the pod won't be called myshell but instead myshell-xyz so you can't shell into myshell
## because it doesn't exist


##Drain a node######
kubectl drain worker-0


###The difference between port and target port
##REF:https://matthewpalmer.net/kubernetes-app-developer/articles/kubernetes-ports-targetport-nodeport-service.html

## obviously NodePort: xxx is the port we use to access our service from outside of our k8s cluster
## "port" is the port that services inside the cluster can communicate on to your svc
## and "targetPort" is the port that the container is actually listening on
## Example

ports:
  - nodePort: 32321
    port: 443
    targetPort: 8443


##The above sample says we can access the svc externally on 32321 i.e. http://worker-0:32321
## the "port" says within the k8s cluster other pods/deployments/anything can access on port 443
## and "targetPort" is the actaully exposed port in the container 8443 so if another pod 
## within the cluster accesses the deployment/pod on 443 it should roll to 8443 on the target pod(s)

####Create a config file for a user

kubectl --kubeconfig john.kubeconfig config set-cluster mycluster --server https://IP_OF_API_SERVER:6443 --certificate-authority=ca.crt
kubectl --kubeconfig john.kubeconfig config set-credentials john --client-certificate /home/john/john.crt -client-key /home/john/john.key
kubectl --kubeconfig john.kubeconfig config set-context john-kubernetes --cluster mycluster --namespace finance --user john

##Note if you look at john.kubeconfig and the current-context is not set you need to set it
current-context: john-kubernetes
##and then save it and give it to john

##Create a role

kubectl create role john-finance --verb=get --verb=list --verb=watch --resource=pods --namespace=finance

##Or

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: finance
  name: john-finance
rules:
- apiGroups: [""] # "" indicates the core API group
  resources: ["pods"]
  verbs: ["get", "watch", "list"]



kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  namespace: finance
  name: john-finance
rules:
- apiGroups: ["", "extensions", "apps"]
  resources: ["deployments", "replicasets", "pods"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"] # You can also use ["*"]
  
####Create a role-binding

kubectl create rolebinding john-finance-rolebinding --role=john-finance --user=john --namespace=finance

##Or

kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: john-finance-rolebinding
  namespace: finance
subjects:
- kind: User
  name: john
  apiGroup: ""
roleRef:
  kind: Role
  name: john-finance
  apiGroup: ""

##Note a role called john-finance has to exist to do this

